{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny GPT-2 implementation"
      ],
      "metadata": {
        "id": "m1iujt-DUGdj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mh1h_AvMvReH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "Om3UsZRmNi-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"lines.txt\"\n",
        "\n",
        "# Read full dataset\n",
        "with open(\"lines.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TLyLurWHyuj",
        "outputId": "8f0e80df-2247-456a-fb94-e568b2ede5f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delicate savage / you'll never hold the cinder / but still you will burn $\n",
            "our destination / the skyline of this city / shining horizon $\n",
            "a splash and a cry /  words pulled from the riverside /  dried\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "bF9us49M7yPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Ensure we use GPT-2-style byte-level pre-tokenizer and decoder\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "Ay4r-xAxAB89"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BPE merges directly on the full corpus file\n",
        "tokenizer.train(\n",
        "    files=[file_path],\n",
        "    vocab_size=1024,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<|endoftext|>\"],\n",
        ")"
      ],
      "metadata": {
        "id": "2waSFPZbAOoX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check on a small slice\n",
        "sample_text = text[:200]\n",
        "sample_ids = tokenizer.encode(sample_text).ids\n",
        "sample_roundtrip = tokenizer.decode(sample_ids)\n",
        "print(\"Roundtrip:\", sample_roundtrip[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKONsQgcAci_",
        "outputId": "159bc407-0289-4f8c-e8ac-d079d6e52e18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roundtrip:  delicate savage / you'll never hold the cinder / but still you will burn $\n",
            "our destination / the skyline of this city / shining horizon $\n",
            "a splash and a cry /  words pulled from the riverside /  drie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode entire corpus into token IDs\n",
        "all_ids = tokenizer.encode(text).ids\n",
        "all_ids = np.array(all_ids, dtype=np.int32)"
      ],
      "metadata": {
        "id": "L-nh0pINAhGt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/validation split at token level (90/10)\n",
        "n_tokens = len(all_ids)\n",
        "split_idx = int(0.9 * n_tokens)\n",
        "train_tokens = all_ids[:split_idx]\n",
        "val_tokens = all_ids[split_idx:]"
      ],
      "metadata": {
        "id": "Okcv7TOqAhME"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling windows"
      ],
      "metadata": {
        "id": "uUAC183ut6E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 64   # context length (tokens per training example)\n",
        "batch_size = 256    # number of sequences per batch\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Return a batch of input (x) and target (y) token windows.\n",
        "    \"\"\"\n",
        "    data = train_tokens if split == \"train\" else val_tokens\n",
        "\n",
        "    # Random starting indices; ensure room for block_size+1 (for targets)\n",
        "    max_start = len(data) - block_size - 1\n",
        "    idx = np.random.randint(0, max_start + 1, size=(batch_size,))\n",
        "\n",
        "    # Build x and y by slicing contiguous windows\n",
        "    x = np.stack([data[i : i + block_size] for i in idx])\n",
        "    y = np.stack([data[i + 1 : i + 1 + block_size] for i in idx])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "t0kwg94W_O1z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration"
      ],
      "metadata": {
        "id": "FNoHPYeeubXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "F5vKx0Xqucwg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "-NbFdS-L6o3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598668ff-cb27-44a2-c187-1b1296394330"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fixed vocab size = 256\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "dSQ50WME6tEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a23293b-4a2c-4cb8-8594-bb8cda1de9cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny GPT-2 Config\n",
        "n_layer = 8      # number of Transformer blocks\n",
        "n_head  = 8      # attention heads per block\n",
        "n_embd  = 512    # embedding (model) dimension\n",
        "dropout = 0.10   # mild regularization\n",
        "\n",
        "# Optimization knobs\n",
        "learning_rate   = 3e-4\n",
        "weight_decay    = 0.01\n",
        "grad_clip       = 1.0\n",
        "warmup_steps    = 300\n",
        "train_steps     = 10000"
      ],
      "metadata": {
        "id": "7EHH7gGn6tJu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "cBxKNDs7euOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GELU activation\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.gelu(x)"
      ],
      "metadata": {
        "id": "jW8jbXCb247D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Masked multi-head self-attention\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0  # must divide evenly\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "\n",
        "        # Combined projection for Q, K, V\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        self.out_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # Causal mask to prevent looking ahead\n",
        "        mask = torch.triu(torch.ones(block_size, block_size), 1)\n",
        "        self.register_buffer(\"mask\", mask == 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape  # (batch, time, channels)\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        qkv = self.qkv(x)  # shape: (B, T, 3*C)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "        # Reshape for multi-head attention (B, T, 3*C)\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention scores\n",
        "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5) # (B, n_head, T, T)\n",
        "\n",
        "        # Apply causal mask (True entries get -inf)\n",
        "        att = att.masked_fill(self.mask[:T, :T], float('-inf'))\n",
        "\n",
        "        # Softmax + droput -> attention weights\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = att @ v  # (B, n_head, T, head_dim)\n",
        "\n",
        "        # Recombine heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "K0oDfn4Fx_GR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed-forward MLP\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "pntipMoIx_JS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = FeedForward(n_embd, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))  # Pre-norm + residual\n",
        "        x = x + self.mlp(self.ln2(x))   # Pre-norm + residual\n",
        "        return x"
      ],
      "metadata": {
        "id": "4z-kFvPvyDLn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full GPT-2 model\n",
        "class GPT2Tiny(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layer, n_head, n_embd, block_size, dropout):\n",
        "        super().__init__()\n",
        "        # Token and positional embeddings\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(n_embd, n_head, block_size, dropout)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # Final normalization\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)  # LM head\n",
        "\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.block_size, \"Input sequence too long!\"\n",
        "\n",
        "        # Embed tokens + positions\n",
        "        tok_emb = self.tok_emb(idx)  # (B, T, n_embd)\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_emb(pos)  # (1, T, n_embd)\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Final layernorm + logits\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        # If training, compute loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "hzKHXZMjyDN9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "model = GPT2Tiny(\n",
        "    vocab_size=vocab_size,\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    dropout=dropout\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "YeFrYwa9yHxm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and scheduler"
      ],
      "metadata": {
        "id": "Vga9IXlHhheq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        ")"
      ],
      "metadata": {
        "id": "KDPn5BNRhfjU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple linear warmup (optional)\n",
        "def get_lr(step):\n",
        "    if step < warmup_steps:\n",
        "        return learning_rate * (step + 1) / warmup_steps\n",
        "    return learning_rate\n"
      ],
      "metadata": {
        "id": "4-IITFE8hj6q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "UKe37XUqhm-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(num_batches=50):\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = []\n",
        "        for _ in range(num_batches):\n",
        "            x, y = get_batch(split)\n",
        "            x = torch.from_numpy(x).long().to(device)\n",
        "            y = torch.from_numpy(y).long().to(device)\n",
        "            _, loss = model(x, y)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "xmHxzzdYhn7x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 200\n",
        "\n",
        "model.train()\n",
        "for step in range(train_steps):\n",
        "    # Adjust learning rate (warmup)\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # Get batch\n",
        "    x, y = get_batch(\"train\")\n",
        "    x = torch.from_numpy(x).long().to(device)\n",
        "    y = torch.from_numpy(y).long().to(device)\n",
        "\n",
        "    # Forward + backward\n",
        "    logits, loss = model(x, y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Logging\n",
        "    if step % eval_interval == 0 or step == train_steps - 1:\n",
        "        losses = estimate_loss(num_batches=20)\n",
        "        print(\n",
        "            f\"step {step}: \"\n",
        "            f\"train loss {losses['train']:.4f}, \"\n",
        "            f\"val loss {losses['val']:.4f}, \"\n",
        "            f\"curr lr {lr:.2e}\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxgFbYXPjcd8",
        "outputId": "15182121-c084-45cd-eab5-a6b6f74b72bf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 7.1528, val loss 7.1543, curr lr 1.00e-06\n",
            "step 200: train loss 4.1681, val loss 4.2587, curr lr 2.01e-04\n",
            "step 400: train loss 3.5274, val loss 3.8028, curr lr 3.00e-04\n",
            "step 600: train loss 2.8819, val loss 3.6060, curr lr 3.00e-04\n",
            "step 800: train loss 1.9406, val loss 3.7879, curr lr 3.00e-04\n",
            "step 1000: train loss 0.7963, val loss 4.3519, curr lr 3.00e-04\n",
            "step 1200: train loss 0.3186, val loss 4.9522, curr lr 3.00e-04\n",
            "step 1400: train loss 0.2382, val loss 5.3016, curr lr 3.00e-04\n",
            "step 1600: train loss 0.2159, val loss 5.5756, curr lr 3.00e-04\n",
            "step 1800: train loss 0.2032, val loss 5.7263, curr lr 3.00e-04\n",
            "step 2000: train loss 0.1917, val loss 5.8078, curr lr 3.00e-04\n",
            "step 2200: train loss 0.1866, val loss 5.9545, curr lr 3.00e-04\n",
            "step 2400: train loss 0.1810, val loss 6.0555, curr lr 3.00e-04\n",
            "step 2600: train loss 0.1758, val loss 6.1495, curr lr 3.00e-04\n",
            "step 2800: train loss 0.1722, val loss 6.2120, curr lr 3.00e-04\n",
            "step 3000: train loss 0.1701, val loss 6.2945, curr lr 3.00e-04\n",
            "step 3200: train loss 0.1666, val loss 6.3771, curr lr 3.00e-04\n",
            "step 3400: train loss 0.1637, val loss 6.4400, curr lr 3.00e-04\n",
            "step 3600: train loss 0.1620, val loss 6.4695, curr lr 3.00e-04\n",
            "step 3800: train loss 0.1598, val loss 6.5542, curr lr 3.00e-04\n",
            "step 4000: train loss 0.1588, val loss 6.5449, curr lr 3.00e-04\n",
            "step 4200: train loss 0.1569, val loss 6.6261, curr lr 3.00e-04\n",
            "step 4400: train loss 0.1560, val loss 6.6587, curr lr 3.00e-04\n",
            "step 4600: train loss 0.1541, val loss 6.7294, curr lr 3.00e-04\n",
            "step 4800: train loss 0.1533, val loss 6.7816, curr lr 3.00e-04\n",
            "step 5000: train loss 0.1519, val loss 6.8782, curr lr 3.00e-04\n",
            "step 5200: train loss 0.1509, val loss 6.8694, curr lr 3.00e-04\n",
            "step 5400: train loss 0.1494, val loss 6.8865, curr lr 3.00e-04\n",
            "step 5600: train loss 0.1496, val loss 6.9447, curr lr 3.00e-04\n",
            "step 5800: train loss 0.1481, val loss 6.9942, curr lr 3.00e-04\n",
            "step 6000: train loss 0.1477, val loss 7.0395, curr lr 3.00e-04\n",
            "step 6200: train loss 0.1461, val loss 7.0186, curr lr 3.00e-04\n",
            "step 6400: train loss 0.1466, val loss 7.1057, curr lr 3.00e-04\n",
            "step 6600: train loss 0.1440, val loss 7.1638, curr lr 3.00e-04\n",
            "step 6800: train loss 0.1453, val loss 7.2029, curr lr 3.00e-04\n",
            "step 7000: train loss 0.1448, val loss 7.1682, curr lr 3.00e-04\n",
            "step 7200: train loss 0.1429, val loss 7.2981, curr lr 3.00e-04\n",
            "step 7400: train loss 0.1423, val loss 7.2894, curr lr 3.00e-04\n",
            "step 7600: train loss 0.1412, val loss 7.3925, curr lr 3.00e-04\n",
            "step 7800: train loss 0.1413, val loss 7.3617, curr lr 3.00e-04\n",
            "step 8000: train loss 0.1411, val loss 7.4089, curr lr 3.00e-04\n",
            "step 8200: train loss 0.1400, val loss 7.4189, curr lr 3.00e-04\n",
            "step 8400: train loss 0.1401, val loss 7.4440, curr lr 3.00e-04\n",
            "step 8600: train loss 0.1407, val loss 7.5551, curr lr 3.00e-04\n",
            "step 8800: train loss 0.1376, val loss 7.5292, curr lr 3.00e-04\n",
            "step 9000: train loss 0.1393, val loss 7.5746, curr lr 3.00e-04\n",
            "step 9200: train loss 0.1382, val loss 7.6276, curr lr 3.00e-04\n",
            "step 9400: train loss 0.1380, val loss 7.6712, curr lr 3.00e-04\n",
            "step 9600: train loss 0.1386, val loss 7.6619, curr lr 3.00e-04\n",
            "step 9800: train loss 0.1364, val loss 7.6961, curr lr 3.00e-04\n",
            "step 9999: train loss 0.1362, val loss 7.7221, curr lr 3.00e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation"
      ],
      "metadata": {
        "id": "v2sgyb3Ljn_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Autoregressive generation with optional temperature and top-k sampling.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop context to block size\n",
        "        idx_cond = idx[:, -model.block_size:]\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(idx_cond)   # (B, T, vocab_size)\n",
        "        logits = logits[:, -1, :]    # (B, vocab_size) â€“ last time step\n",
        "\n",
        "        # Temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Optional top-k filtering\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            threshold = v[:, [-1]]\n",
        "            logits[logits < threshold] = -float('inf')\n",
        "\n",
        "        # Sample next token\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "\n",
        "        # Append\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "q0o4e8ESjnbt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_new_tokens=50, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Convenience wrapper: takes a text prompt and returns generated text.\n",
        "    Works with `tokenizers.Tokenizer`.\n",
        "    \"\"\"\n",
        "    # Encode prompt -> Encoding object\n",
        "    encoding = tokenizer.encode(prompt)\n",
        "    input_ids = encoding.ids  # list[int]\n",
        "\n",
        "    # Make tensor on the right device\n",
        "    idx = torch.tensor([input_ids], dtype=torch.long, device=device)  # shape (1, T)\n",
        "\n",
        "    # Generate\n",
        "    out = generate(\n",
        "        model,\n",
        "        idx,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "\n",
        "    # Decode full sequence (prompt + generated tokens)\n",
        "    generated_ids = out[0].tolist()\n",
        "    return tokenizer.decode(generated_ids)"
      ],
      "metadata": {
        "id": "jTiC5mDnjneZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage after training:\n",
        "sample_prompt = \"Once upon a time\"\n",
        "print(\"Generated text:\\n\")\n",
        "print(generate_text(sample_prompt, max_new_tokens=60, temperature=0.7, top_k=50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0tVhT2vjnjq",
        "outputId": "854bd42c-fc53-41da-be92-b46ad81123f4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "\n",
            " Once upon a time $\n",
            "breathe in the fresh air / crisp and cool spring is now here / bright rosy red cheeks $\n",
            "in my room the walls / breathe and the windows blink but / the door remains\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = \"round and round we go\"\n",
        "print(\"Generated text:\\n\")\n",
        "print(generate_text(sample_prompt, max_new_tokens=60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzc5OTyxYWus",
        "outputId": "1a5359b4-af54-462a-d8e3-63cd01c61d8a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "\n",
            " round and round we go / of all it takes is time $\n",
            "we are one family / knight errants of the divine / forever ever more $\n",
            "stop falling in love / it is not good for the soul / loneliness is god $\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcmaXcoZYe2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}